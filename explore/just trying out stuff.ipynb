{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2244c6a6-4006-4ed2-9907-cd02fe1ed0ac",
   "metadata": {},
   "source": [
    "Shoud we only predict the authorId or authorName shoudl be also predicted. Becuase Id and Name are interrelated. One unique name per one unique Id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b17a5ba6-33ef-4a0b-a7bc-73e6500d607b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/Ulyasha/opt/anaconda3/lib/python3.9/site-packages (1.1.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/Ulyasha/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/Ulyasha/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /Users/Ulyasha/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/Ulyasha/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.21.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5437c29-53a6-4b1e-804f-f95634fa5bd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12244/2844348181.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a17d1ce-9513-419d-927d-b383067e8627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "paperId       object\n",
       "title         object\n",
       "authorId       int64\n",
       "authorName    object\n",
       "abstract      object\n",
       "year           int64\n",
       "venue         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data =open(\"Data/train.json\")\n",
    "train_i=json.load(train_data)\n",
    "print(len(train_i))\n",
    "df = pd.read_json(\"Data/train.json\")\n",
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64a7c5f-2c9e-4cf6-8e0e-3518bc2a2992",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "826c01ca-377f-429a-9893-cd88636849f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Ulyasha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6971218-cb5e-468a-b111-854e19524a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "venue         object\n",
       "year           int64\n",
       "abstract      object\n",
       "authorName    object\n",
       "authorId       int64\n",
       "title         object\n",
       "paperId       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#did this becuase I thought I can one-hot encode \"venue\" only. It should come as a first column if you one hot encode it only. \n",
    "cols = list(df.columns)\n",
    "cols.reverse()\n",
    "df_new=df[cols]\n",
    "df_new.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f3befa-be88-438d-83aa-5ffe58410956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizeing abstract and title columns. But what do we have to do after this?\n",
    "df_new['tokenized_abstract'] = df_new.apply(lambda row: nltk.word_tokenize(row['abstract']), axis=1)\n",
    "df_new['tokenized_title'] = df_new.apply(lambda row: nltk.word_tokenize(row['title']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f45e3ea0-d78e-4cf2-aa74-df38c7d19423",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.drop(columns=[\"title\",\"abstract\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361bc064-276e-48b6-80a9-868b818bdbe1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d2bb4ce0-c024-47d4-b31a-983195440636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#METHOD 1. Not sure do we have to get dummies for PaperId (no dublicates, each PaperId is unique) and AuthorName? \n",
    "df_ohe = df_new\n",
    "categorical_columns = [\"venue\"]\n",
    "for col in categorical_columns:\n",
    "    col_ohe = pd.get_dummies(df_new[col], prefix=col)\n",
    "    df_ohe = pd.concat((df_ohe, col_ohe), axis=1).drop(col, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdd4871-3cd0-493c-a974-d822216b7723",
   "metadata": {},
   "source": [
    "#METHOD 2\n",
    "#DOES NOT PERFORM AS PLANNED\n",
    "\n",
    "categorical_cols = [\"venue\"] \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# instantiate labelencoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "# apply le on categorical feature columns\n",
    "df_new[categorical_cols] = df_new[categorical_cols].apply(lambda col: le.fit_transform(col))    \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "#One-hot-encode the categorical columns.\n",
    "#Unfortunately outputs an array instead of dataframe.\n",
    "array_hot_encoded = ohe.fit_transform(df_new[categorical_cols])\n",
    "\n",
    "#Convert it to df\n",
    "data_hot_encoded = pd.DataFrame(array_hot_encoded, index=df_new.index)\n",
    "\n",
    "#Extract only the columns that didnt need to be encoded\n",
    "data_other_cols = df_new.drop(columns=categorical_cols)\n",
    "\n",
    "#Concatenate the two dataframes : \n",
    "data = pd.concat([data_hot_encoded, data_other_cols], axis=1)\n",
    "data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3300369e-803d-4a85-be7e-c0586c5cd3fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ohe.columns.get_loc(\"authorId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c074cd3b-760e-478c-9929-af7ddee05a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f8771b8-4de5-48f7-a065-b040de14735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[lambda x: x.index != 2].values\n",
    "y = df.iloc[:, 2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc456d5f-6380-443b-8c1c-f801338ee972",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size= 0.4)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a77a9ec-1138-4c3b-ba76-eb835401e5da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b27fedf4bc35f9aad9c2542d0deb6e37f774d7c028b471ec4ea6bd7d621b37d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
