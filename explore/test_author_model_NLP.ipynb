{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_json(\"../data/raw/train.json\")\n",
    "print(df_train.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            authorName   0\n",
      "835          Chuhan Wu  13\n",
      "3926    Ryan Cotterell  13\n",
      "1889        Ivan Vulic  13\n",
      "3789     Ramit Sawhney  12\n",
      "3991        S. Malmasi  12\n",
      "...                ...  ..\n",
      "1535    Gonzalo MÃ©ndez   1\n",
      "3742    R. Rajalakshmi   1\n",
      "1534  Gonzalo Iglesias   1\n",
      "3744       R. SarathP.   1\n",
      "2755         M. Becker   1\n",
      "\n",
      "[5511 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#grouping by authors\n",
    "a = df_train.groupby(['authorName']).size().reset_index().sort_values(0,ascending=False)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chuhan wu is most common, make a test model for her"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_chuhan = df_train[df_train['authorName']=='Chuhan Wu'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_chuhan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_chuhan_NLP = df_train_chuhan[['title', 'abstract']]\n",
    "df_train_chuhan_NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>venue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>2021</td>\n",
       "      <td>FINDINGS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934</th>\n",
       "      <td>2019</td>\n",
       "      <td>NAACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2815</th>\n",
       "      <td>2018</td>\n",
       "      <td>Fig-Lang@NAACL-HLT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5175</th>\n",
       "      <td>2018</td>\n",
       "      <td>*SEMEVAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5311</th>\n",
       "      <td>2021</td>\n",
       "      <td>FINDINGS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5576</th>\n",
       "      <td>2018</td>\n",
       "      <td>*SEMEVAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6271</th>\n",
       "      <td>2019</td>\n",
       "      <td>EMNLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8461</th>\n",
       "      <td>2022</td>\n",
       "      <td>ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8721</th>\n",
       "      <td>2019</td>\n",
       "      <td>EMNLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9844</th>\n",
       "      <td>2018</td>\n",
       "      <td>EMNLP 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9874</th>\n",
       "      <td>2019</td>\n",
       "      <td>ACL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10304</th>\n",
       "      <td>2020</td>\n",
       "      <td>FINDINGS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11771</th>\n",
       "      <td>2019</td>\n",
       "      <td>EMNLP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       year               venue\n",
       "655    2021            FINDINGS\n",
       "1934   2019               NAACL\n",
       "2815   2018  Fig-Lang@NAACL-HLT\n",
       "5175   2018            *SEMEVAL\n",
       "5311   2021            FINDINGS\n",
       "5576   2018            *SEMEVAL\n",
       "6271   2019               EMNLP\n",
       "8461   2022                 ACL\n",
       "8721   2019               EMNLP\n",
       "9844   2018          EMNLP 2018\n",
       "9874   2019                 ACL\n",
       "10304  2020            FINDINGS\n",
       "11771  2019               EMNLP"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_chuhan_place = df_train_chuhan[['year', 'venue']]\n",
    "df_train_chuhan_place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - We use bag-of-words technique on our first model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected the first abstract\n",
    "df_train_chuhan_NLP_abst1 = [df_train_chuhan_NLP.iloc[0,1]]\n",
    "df_train_chuhan_NLP_abst1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(input='content')\n",
    "\n",
    "# our sample set is df_train_chuhan_NLP_abst1\n",
    "# fit the bag-of-words model\n",
    "\n",
    "bag = vectorizer.fit_transform(df_train_chuhan_NLP_abst1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get unique words / tokens found in all the documents. The unique words / tokens represents the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate the indices with each unique word\n",
    "\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 4 1 1 1 1 1 1 1 6 2 1 1 1 1\n",
      "  1 1 1 6 1 1 1 1 1 1 1 1 3 1 2 1 1 7 1 3 1 1 1 2 1 2 1 1 1 1 6 6 3 2 1 1\n",
      "  1 1 1 1 2 1 6 1 1 2 4 1 1 1 8 2 3 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Print the numerical feature vector\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test with the second abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_chuhan_NLP_abst2 = [df_train_chuhan_NLP.iloc[1,1]]\n",
    "df_train_chuhan_NLP_abst2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  1  2  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  2  1  0  0  1\n",
      "   0  3  1  1  1  0  4  6  0  0  0  0  0  1  0  3  0  0  1  0  0  0  0  0\n",
      "   0  3  0  0  0  0  0  2  2  0  0  3  1  0  0  0  1  0  0  0  1  1  0  1\n",
      "   0  0  0  0  1  0  7  0  0  1  7  0  0  0  8  0  3  0  2]]\n"
     ]
    }
   ],
   "source": [
    "testbag = vectorizer.transform(df_train_chuhan_NLP_abst2)\n",
    "print(testbag.toarray())\n",
    "#print(testbag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(testbag.count_nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "print(bag.count_nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3516483516483517\n"
     ]
    }
   ],
   "source": [
    "score = testbag.count_nonzero()/ bag.count_nonzero()\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Now we use all available texts to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list with all abstracts and titles\n",
    "df_train_chuhan_NLP_abstracts = df_train_chuhan_NLP.iloc[:,1].tolist()\n",
    "df_train_chuhan_NLP_titles = df_train_chuhan_NLP.iloc[:,0].tolist()\n",
    "df_train_chuhan_NLP_all = df_train_chuhan_NLP_titles + df_train_chuhan_NLP_abstracts\n",
    "\n",
    "print(df_train_chuhan_NLP_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_all = CountVectorizer(input='content')\n",
    "\n",
    "# our sample set is df_train_chuhan_NLP_abst1\n",
    "# fit the bag-of-words model\n",
    "\n",
    "bag = vectorizer_all.fit_transform(df_train_chuhan_NLP_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer_all.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Same_author = df_train_chuhan_NLP.iloc[5,:].tolist()\n",
    "test_Same_author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bag_test_Same_author = vectorizer_all.transform(test_Same_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08921644685802949\n"
     ]
    }
   ],
   "source": [
    "score = bag_test_Same_author.count_nonzero()/ bag.count_nonzero()\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing with another author Ryan Cotterell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ryan = df_train[df_train['authorName']=='Ryan Cotterell']\n",
    "df_train_ryan_NLP = df_train_ryan[['title', 'abstract']]\n",
    "ryan_sent1 = df_train_ryan_NLP.iloc[6,:].tolist()\n",
    "print(ryan_sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03413498836307215\n"
     ]
    }
   ],
   "source": [
    "bag_ryan = vectorizer_all.transform(ryan_sent1)\n",
    "score2 = bag_ryan.count_nonzero()/ bag.count_nonzero()\n",
    "print(score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_ryan.count_nonzero()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_test_Same_author2 = vectorizer_all.transform(test_Same_author2)\n",
    "score2 = bag_test_Same_author2.count_nonzero()/ bag.count_nonzero()\n",
    "print(score2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b27fedf4bc35f9aad9c2542d0deb6e37f774d7c028b471ec4ea6bd7d621b37d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
